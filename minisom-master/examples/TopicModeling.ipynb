{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will see how to perform topic extraction using MiniSom. The goal is to extract the main topics (represented as a set of words) that occur in a collection of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from minisom import MiniSom\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The colloction of documents that we will work with is the famous `20newsgroups` dataset. It contains more than 10000 newsgroups posts. We will download the dataset using sklearn and will transform the textual documents into a matrix `D` where each row represents a post using <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer\">TF-IDF representation</a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=no_features,\n",
    "                                   stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "D = tfidf.todense().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to train a SOM that clusters the documents, the total number of neurons in the SOM will be also the number of topics to extract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ 5000 / 5000 ] 100% - 0:00:00 left \n",
      " quantization error: 0.9874395767896555\n"
     ]
    }
   ],
   "source": [
    "n_neurons = 2\n",
    "m_neurons = 4\n",
    "som = MiniSom(n_neurons, m_neurons, no_features)\n",
    "som.random_weights_init(D)\n",
    "som.train(D, 5000, random_order=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider as topic the list of first `top_keywords` associated with the biggest weights of each neuron. With the following for loop we will inspect all the weights and recover the words associated with the weights using the feature names saved by the TfidfVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 : used groups idea month said guns police gun group hell\n",
      "Topic 2 : cost 12 sound 30 lines total point pc 100 max\n",
      "Topic 3 : way hi widget thought hockey better running time know times\n",
      "Topic 4 : computer chips happens control alt escrow apple text chip key\n",
      "Topic 5 : just offer command manager os chicago files file dos windows\n",
      "Topic 6 : heard couple people basic apparently just men results ve questions\n",
      "Topic 7 : cause turkish send address players likely goal lost government team\n",
      "Topic 8 : action think paul don body bad christians jesus christ god\n"
     ]
    }
   ],
   "source": [
    "top_keywords = 10\n",
    "\n",
    "weights = som.get_weights()\n",
    "cnt = 1\n",
    "for i in range(n_neurons):\n",
    "    for j in range(m_neurons):\n",
    "        keywords_idx = np.argsort(weights[i,j,:])[-top_keywords:]\n",
    "        keywords = ' '.join([tfidf_feature_names[k] for k in keywords_idx])\n",
    "        print('Topic', cnt, ':', keywords)\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
